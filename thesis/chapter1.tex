\chapter{ТЕОРЕТИЧЕСКИЕ ОСНОВЫ ПОСТРОЕНИЯ РАСПРЕДЕЛЁННЫХ СИСТЕМ}

\section{Формальная модель вычислительной среды}

Перед формулировкой конкретных алгоритмических задач необходимо определить математическую абстракцию среды, в которой функционирует распределённая система. В общем виде распределённая вычислительная система $S$ определяется как совокупность конечного множества процессов (узлов) $\Pi = \{p_1, p_2, \dots, p_n\}$, объединенных коммуникационной сетью.

Каждый процесс $p_i$ представляет собой детерминированный автомат, обладающий локальной памятью и процессором. Критической особенностью архитектуры является отсутствие общей памяти. Взаимодействие между узлами осуществляется исключительно посредством передачи сообщений $m \in M$ через каналы связи.

\subsection{Временная модель}

Фундаментальной проблемой распределенных систем является отсутствие единого глобального времени. Физические часы на разных узлах не могут быть идеально синхронизированы в силу неизбежного расхождения (дрейфа) хода локальных часов. В связи с этим, понятие «одновременности» событий для географически удаленных узлов является неопределенным.

В зависимости от предсказуемости временных задержек в сети и на узлах, выделяют три класса систем. Выбор модели фундаментально влияет на возможность построения отказоустойчивых алгоритмов.

\subsubsection{Синхронная модель}
Система считается синхронной, если существуют известные фиксированные границы (верхние пределы) для:
\begin{itemize}
    \item Времени доставки сообщения по сети ($\Delta_{net}$).
    \item Времени выполнения одного шага вычислений процессом ($\Delta_{proc}$).
    \item Расхождения локальных часов ($\Delta_{clock}$).
\end{itemize}
В такой среде любой сбой обнаруживается тривиально по тайм-ауту. Однако для реальных сетей (IP-сетей) эта модель неадекватна из-за вероятностного характера очередей маршрутизаторов.

\subsubsection{Асинхронная модель}
В асинхронной модели отсутствуют какие-либо гарантии временных интервалов:
\begin{itemize}
    \item Сообщения могут задерживаться на неопределенное время.
    \item Относительная скорость процессоров может различаться на порядки.
\end{itemize}
В такой среде невозможно отличить медленный узел или узел с плохой связью от узла, который полностью вышел из строя. Это накладывает теоретический запрет на решение многих задач координации (см. далее теорему FLP).

\subsubsection{Модель частичной синхронности}
Данная модель является компромиссной и наиболее применимой для проектирования надежных систем хранения. Она предполагает существование неизвестного момента времени $GST$ (Global Stabilization Time), после наступления которого система начинает вести себя как синхронная. 
То есть:
\begin{equation}
    \exists \Delta, GST : \forall t > GST \implies \text{задержка} \le \Delta
\end{equation}
Это допущение позволяет алгоритмам гарантировать безопасность всегда, а свойство завершаемости (прогресс работы) — только после стабилизации сети.

\subsection{Модель коммуникационных каналов}

Каналы связи между процессами $p_i$ и $p_j$ в рамках данной работы моделируются как ненадежные асинхронные каналы с потерями (\textit{Fair-loss links}). 

Характеристики каналов:
\begin{enumerate}
    \item \textbf{Отсутствие гарантии доставки:} Сообщение, отправленное процессом-отправителем, может быть потеряно сетью и никогда не достигнуть получателя.
    \item \textbf{Дублирование:} Одно сообщение может быть доставлено получателю более одного раза.
    \item \textbf{Переупорядочивание:} Последовательность доставки сообщений не гарантирует сохранение порядка их отправки.
    \item \textbf{Отсутствие повреждений:} Если сообщение доставлено, его содержимое соответствует отправленному. 
\end{enumerate}

\subsection{Классификация аварийных ситуаций}

Надежность распределенной системы определяется её способностью функционировать в условиях частичных отказов. Множество возможных сбоев:

\begin{itemize}
    \item \textbf{Отказ-остановка (Crash-stop):} Процесс работает корректно, но в произвольный момент может прекратить выполнение. После остановки процесс никогда не возвращается в строй.
    \item \textbf{Отказ с восстановлением (Crash-recovery):} Процесс может остановиться (потеряв содержимое оперативной памяти), а спустя некоторое время возобновить работу. Для корректного функционирования в этой модели процесс обязан иметь доступ к стабильному хранилищу, данные в котором переживают сбой.
    \item \textbf{Византийский сбой (Byzantine failure):} Процесс может вести себя произвольно, в том числе отправлять противоречивые или злонамеренно сформированные данные, пытаясь нарушить работу системы.
\end{itemize}

В дальнейших главах при постановке задачи будет использоваться комбинация модели \textbf{частичной синхронности} и модели отказов \textbf{Crash-recovery}, так как они наиболее точно описывают условия эксплуатации современных центров обработки данных.

\section{Формальная постановка задачи распределённого консенсуса}

В условиях асинхронной среды с ненадежными каналами связи возникает необходимость координации действий независимых процессов. Задача консенсуса является фундаментальной примитивной задачей в теории распределённых вычислений.

В наиболее общем виде она формулируется так: множество процессов должно прийти к соглашению относительно некоторого значения данных, предложенного одним из участников, несмотря на то, что часть процессов может отказать в произвольный момент времени.

\subsection{Спецификация свойств корректности}

Корректность любого алгоритма консенсуса определяется через выполнение набора формальных свойств. В академической литературе эти свойства принято делить на два класса: свойства безопасности (Safety) и свойства живости (Liveness).

Алгоритм решает задачу консенсуса, если для предложенных значений выполняются следующие условия:

\begin{enumerate}
    \item \textbf{Согласованность (Agreement):}
    Критически важное свойство безопасности. Оно требует, чтобы никакие два исправных процесса не могли принять различные результирующие значения. Если один узел решил, что согласованным значением является $A$, то любой другой узел в системе может прийти к решению только $A$.
    Нарушение этого свойства ведет к нарушению целостности данных (Split-brain), когда разные узлы считают истинными разные версии истории.

    \item \textbf{Действительность (Validity):}
    Свойство, исключающее тривиальные или случайные решения. Если алгоритм принял решение $v$, то это значение гарантированно должно было быть предложено одним из участников системы.

    \item \textbf{Необратимости (Integrity):}
    Процесс не может изменить свое решение после того, как оно было принято.

    \item \textbf{Завершаемость (Termination):}
    Свойство живости, требующее, чтобы каждый исправный процесс в конечном итоге (за конечное время) принял какое-либо решение. Иными словами, алгоритм не должен зависать в бесконечных циклах ожидания или перевыборов.
\end{enumerate}

\subsection{Принцип кворумного согласования}

Для обеспечения свойства Согласованности (Safety) в условиях возможного разделения сети, алгоритмы консенсуса опираются на концепцию кворумов.

Кворум $Q$ определяется как подмножество процессов, участие которых необходимо и достаточно для ратификации какого-либо решения (выбора значения или смены фазы протокола). Математическим фундаментом надежности кворумов является свойство пересечения: любые два кворума $Q_1$ и $Q_2$, собранные в системе, обязаны иметь непустое пересечение.
\[ Q_1 \cap Q_2 \neq \emptyset \]

Наличие общего элемента (узла-свидетеля) в любых двух кворумах гарантирует, что информация о принятом решении будет сохранена и передана следующим этапам алгоритма, предотвращая принятие противоречащих решений.

\textbf{Кворум большинства}
Наиболее распространенным способом реализации свойства пересечения является требование большинства голосов. Если общее число процессов в системе равно $N$, то размер кворума устанавливается как строгое большинство:
\[ |Q| = \lfloor \frac{N}{2} \rfloor + 1 \]

\textbf{Отказоустойчивость}
Использование механизма большинства определяет границы надежности системы. Для того чтобы система могла продолжать сбор кворума (сохранять свойство Liveness) при одновременном отказе $f$ процессов, общее размер ансамбля должен составлять не менее $2f + 1$ узлов.

Например, для кластера из 5 узлов кворум равен 3, и система сохраняет работоспособность при отказе любых 2 узлов.

\section{Теоретические ограничения разрешимости}

При проектировании распределённых систем необходимо учитывать фундаментальные границы вычислимости. Существует ряд математически доказанных утверждений, постулирующих невозможность создания идеального алгоритма консенсуса в определенных моделях среды.

\subsection{Теорема невозможности FLP}

В 1985 году Фишер, Линч и Патерсон сформулировали важнейший результат теории распределённых вычислений, известный как \textbf{теорема FLP} (Fischer, Lynch, Paterson Impossibility Result).

Теорема утверждает, что в асинхронной распределённой системе не существует детерминированного алгоритма консенсуса, который гарантированно удовлетворял бы трём свойствам одновременно:
\begin{enumerate}
    \item \textbf{Согласованность} (все узлы выбирают одно значение).
    \item \textbf{Действительность} (выбирается предложенное значение).
    \item \textbf{Завершаемость} (алгоритм завершается за конечное время).
\end{enumerate}
Это ограничение справедливо даже при допущении, что в системе может отказать (crash failure) всего один процесс, а каналы связи абсолютно надежны.

\textbf{Причина ограничения:}
Корень проблемы кроется в невозможности различить в асинхронной среде две ситуации:
\begin{itemize}
    \item Процесс окончательно вышел из строя.
    \item Процесс работает экстремально медленно или сообщение от него задерживается в сети на неопределенно долгий срок.
\end{itemize}
Вследствие этой неопределённости, любой детерминированный алгоритм может попасть в состояние, в котором он будет бесконечно ожидать сообщения от <<подозрительного>> узла, не имея возможности принять окончательное решение без риска нарушить согласованность.

Поскольку свойства Cогласованности и Действительности являются обязательными для корректности данных, на практике приходится жертвовать детерминированной Завершаемостью. Алгоритмы (в том числе Raft) обходят ограничение FLP, используя модель \textbf{частичной синхронности}: они гарантируют безопасность всегда, но гарантируют прогресс (liveness) только в периоды стабильной работы сети.

\subsection{Теорема CAP}

Второе фундаментальное ограничение касается эксплуатационных характеристик распределённых хранилищ и формулируется теоремой Брюера (CAP-теорема). Она гласит, что распределённая система не может одновременно обеспечивать следующие три гарантии:

\begin{enumerate}
    \item \textbf{Согласованность (Consistency):}
    Каждое чтение получает самую последнюю записанную версию данных или ошибку. С точки зрения внешнего наблюдателя система ведет себя как единственный (неразделенный) узел.
    
    \item \textbf{Доступность (Availability):}
    Каждый запрос к исправному узлу получает успешный отклик (не ошибку), однако без гарантии, что этот отклик содержит самую свежую версию данных.
    
    \item \textbf{Устойчивость к разделению (Partition Tolerance):}
    Система продолжает функционировать, несмотря на потерю произвольного числа сообщений или разрыв связности сети между узлами.
\end{enumerate}

Поскольку в распределённых сетях невозможно гарантировать отсутствие разрывов связи, архитектурный выбор всегда сводится к компромиссу между Согласованностью (CP) и Доступностью (AP).

Алгоритмы строгого консенсуса относятся к классу \textbf{CP-систем}. В случае возникновения сетевого разделения, когда невозможно собрать кворум, система переходит в режим отказа в обслуживании (жертвует доступностью), чтобы предотвратить расхождение данных и гарантировать их строгую согласованность.

\section{Эволюция алгоритмов и сравнительный анализ}

Исторически первым и наиболее влиятельным решением задачи асинхронного консенсуса является протокол \textbf{Paxos}, предложенный Лесли Лэмпортом в 1989 году. На протяжении десятилетий Paxos считался синонимом распределённого консенсуса и служил эталоном, с которым сравнивались все остальные решения.

Однако при попытке применения Paxos для построения практических систем инженеры сталкиваются с рядом фундаментальных препятствий.

\subsection{Проблематика алгоритма Paxos}

Классический алгоритм (часто называемый \textit{Single-Decree Paxos}) обеспечивает математически доказанную безопасность, однако обладает характеристиками, затрудняющими его реализацию.

\textbf{1. Проблема единичного значения} \\
В своей канонической форме Paxos предназначен для согласования \textbf{одного единственного значения}. Как только значение выбрано, оно фиксируется навсегда.
Однако для реализации практических систем требуется согласование непрерывного потока команд (журнала операций). Для этого необходимо расширение протокола, известное как \textbf{Multi-Paxos}.
Существенная проблема заключается в том, что Multi-Paxos не имеет единой строго формализованной спецификации. В оригинальных работах механизм перехода от одиночного значения к потоку описан поверхностно. Это вынуждает разработчиков самостоятельно проектировать критически важные механизмы (оптимизацию потока сообщений, усечение журнала, смену конфигурации), что часто приводит к появлению ошибок в сложных краевых случаях.

\textbf{2. Когнитивная сложность} \\
Второй критической проблемой Paxos является его сложность для восприятия. Архитектура Paxos, основанная на взаимодействии ролей (Proposers, Acceptors, Learners) без явно выделенного постоянного лидера, трудна для интуитивного понимания. Из-за отсутствия простого и однозначного алгоритма реализации Multi-Paxos, создатели реальных систем вынуждены разрабатывать собственные модификации протокола, которые отличаются от теоретически проверенной модели, ставя под угрозу надежность системы.

\subsection{Алгоритм Raft}

Сложность реализации Paxos привела к необходимости создания нового алгоритма, который обладал бы той же надежностью, но был бы пригоден для инженерного применения. Таким решением стал алгоритм \textbf{Raft}, представленный Д. Онгаро и Дж. Оустерхаутом в 2014 году.

Основной целью дизайна Raft было обеспечение свойства \textbf{понятности}. Для достижения этого была применена стратегия декомпозиции: монолитная задача консенсуса была разделена на три слабосвязанных подзадачи:

\begin{enumerate}
    \item \textbf{Выборы лидера (Leader Election):} В отличие от симметричного Paxos, Raft использует модель сильного лидерства. Все запросы клиентов обрабатываются исключительно лидером.
    \item \textbf{Репликация журнала (Log Replication):} Лидер принимает команды от клиентов, добавляет их в свой журнал и реплицирует на другие узлы, навязывая им своё состояние.
    \item \textbf{Безопасность (Safety):} Набор ограничений, гарантирующий корректность данных при смене лидера.
\end{enumerate}

Данная архитектура позволяет рассматривать, реализовывать и отлаживать каждый компонент алгоритма независимо, что существенно снижает вероятность программных ошибок.
